"""
Metric diff computation for regression detection.
"""

from dataclasses import dataclass
from typing import Optional

from .runner import RunResult


@dataclass
class MetricDiff:
    """Difference between baseline and candidate for a single metric."""
    suite: str
    metric: str
    baseline_value: Optional[float]
    candidate_value: Optional[float]
    delta: Optional[float]
    delta_pct: Optional[float]
    higher_is_worse: bool

    def is_regression(self) -> bool:
        """Check if this represents a regression."""
        if self.delta is None:
            return False
        if self.higher_is_worse:
            return self.delta > 0
        else:
            return self.delta < 0

    def to_dict(self) -> dict:
        return {
            'suite': self.suite,
            'metric': self.metric,
            'baseline_value': self.baseline_value,
            'candidate_value': self.candidate_value,
            'delta': self.delta,
            'delta_pct': self.delta_pct,
            'higher_is_worse': self.higher_is_worse,
            'is_regression': self.is_regression()
        }


class DiffEngine:
    """
    Computes metric diffs between baseline and candidate results.
    """

    # Metrics where higher values indicate worse safety
    HIGHER_IS_WORSE = {
        'violation_rate', 'hedging_rate', 'attack_success_rate',
        'delayed_failure_rate', 'policy_erosion_slope',
        'max_drift', 'avg_drift', 'erosion_slope'
    }

    # Metrics where lower values indicate worse safety
    LOWER_IS_WORSE = {
        'avg_first_failure', 'avg_detection_turn',
        'avg_first_failure_turn'
    }

    def compute_diffs(self, run_result: RunResult) -> list[MetricDiff]:
        """
        Compute all metric diffs from a run result.

        Args:
            run_result: Complete regression run results

        Returns:
            List of MetricDiff objects
        """
        diffs = []

        for suite in run_result.suites:
            baseline = run_result.baseline_results[suite]
            candidate = run_result.candidate_results[suite]

            # Get all metrics from both
            all_metrics = set(baseline.metrics.keys()) | set(candidate.metrics.keys())

            for metric in all_metrics:
                baseline_val = baseline.metrics.get(metric)
                candidate_val = candidate.metrics.get(metric)

                # Determine direction
                higher_is_worse = metric in self.HIGHER_IS_WORSE

                # Compute delta
                if baseline_val is not None and candidate_val is not None:
                    delta = candidate_val - baseline_val
                    delta_pct = (delta / baseline_val * 100) if baseline_val != 0 else None
                else:
                    delta = None
                    delta_pct = None

                diffs.append(MetricDiff(
                    suite=suite,
                    metric=metric,
                    baseline_value=baseline_val,
                    candidate_value=candidate_val,
                    delta=delta,
                    delta_pct=delta_pct,
                    higher_is_worse=higher_is_worse
                ))

        return diffs

    def get_regressions(self, diffs: list[MetricDiff]) -> list[MetricDiff]:
        """Filter to only regressions."""
        return [d for d in diffs if d.is_regression()]

    def get_improvements(self, diffs: list[MetricDiff]) -> list[MetricDiff]:
        """Filter to only improvements."""
        return [d for d in diffs if d.delta is not None and not d.is_regression() and d.delta != 0]

    def summarize(self, diffs: list[MetricDiff]) -> dict:
        """Generate summary statistics."""
        regressions = self.get_regressions(diffs)
        improvements = self.get_improvements(diffs)

        return {
            'total_metrics': len(diffs),
            'regressions': len(regressions),
            'improvements': len(improvements),
            'neutral': len(diffs) - len(regressions) - len(improvements),
            'regressed_metrics': [d.metric for d in regressions],
            'improved_metrics': [d.metric for d in improvements]
        }
